#!/usr/bin/env python3
#
# Copyright 2020 ScyllaDB
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import argparse
import re
import os
import sys
import time
import subprocess
import urllib.request
import urllib.error
from pathlib import Path


raid_script = "/opt/scylladb/scripts/scylla_raid_setup"
raid_device = "/dev/md%d"
scylla_root = ""

def scylla_directory(role):
    if role == "all":
        return scylla_root
    else:
        return os.path.join(scylla_root, role)


def curl_instance_data(url):
    max_retries = 5
    retries = 0
    while True:
        try:
            req = urllib.request.Request(url)
            return urllib.request.urlopen(req).read().decode("utf-8")
        except urllib.error.HTTPError:
            print("Failed to grab %s..." % url)
            time.sleep(5)
            retries += 1
            if retries >= max_retries:
                raise


def find_disk(disks, line):
    for disk in disks:
        if line.find(disk) == -1:
            return False
    return True


def config_array(disks, role, mdidx):
    # Is it already constructed
    disks.sort()
    md_state_path = Path("/proc/mdstat")
    with open(md_state_path) as mdstate:
        for l in mdstate:
            if find_disk(disks, l):
                dev = re.search(r"^md\w+", l).group()
                print("Found existing RAID %s, will mount it" % dev)
                subprocess.check_call(["mount", "-o", "noatime",
                                       "/dev/%s" % dev,
                                       scylla_directory(role)])
                return
    print("RAID Array containing %s not found. Creating..." % str(disks))
    disk_devs = ['/dev/%s' % x for x in disks]
    subprocess.run([raid_script, "--raiddev",
                    raid_device % mdidx, "--disks", ",".join(disk_devs),
                    "--root", scylla_root,
                    "--volume-role", role,
                    "--update-fstab"], check=True)


def xenify(devname):
    dev = curl_instance_data('http://169.254.169.254/latest/meta-data/block-device-mapping/' + devname)
    return dev.replace("sd", "xvd")


def device_exists(dev):
    return os.path.exists("/dev/%s" % dev)


def device_is_busy(dev):
    try:
        fd = os.open(dev, os.O_RDWR | os.O_EXCL)
        os.close(fd)
        return False
    except OSError:
        return True


# While testing this, I found the following issue at AWS:
#
# $ ls /dev/nvme*
# /dev/nvme0  /dev/nvme0n1  /dev/nvme1  /dev/nvme1n1
#
# $ curl http://169.254.169.254/latest/meta-data/block-device-mapping/
# ami
# ebs2
# ephemeral0
# root
#
# As one can see, only one of the ephemeral devices were listed.
#
# I saw this happening only on i3 machines, if EBS were listed before
# ephemeral during creation time. However, in that scenario, I saw it
# happening every time I tested.
#
# More info at:
# https://forums.aws.amazon.com/thread.jspa?threadID=250553
#
# So for nvme devices, we'll just scan the device list and see what we
# find. Since the goal is to differentiate between ephemeral and
# non-ephemeral anyway, and NVMe are always ephemeral, this is
# acceptable
def get_disk_bundles():
    # define preferred disk roles. We'll see soon if we can respect them.
    role = {
        "ebs": "unused",
        "ephemeral": "all"
    }

    # Find disk assignments
    devmap = curl_instance_data('http://169.254.169.254/latest/meta-data/block-device-mapping/')
    typemap = {}
    devname = re.compile("^\D+")
    nvme_re = re.compile(r"nvme\d+n\d+$")
    nvmes_present = list(filter(nvme_re.match, os.listdir("/dev")))
    nvmes_free = [nvme for nvme in nvmes_present if not device_is_busy(os.path.join('/dev/', nvme))]

    if nvmes_free:
        typemap["ephemeral"] = nvmes_free

    for dev in devmap.splitlines():
        if dev == "ami" or dev == "root":
            continue

        t = devname.match(dev).group()
        if role[t] == "unused":
            continue

        if t == "ephemeral" and nvmes_present:
            continue

        if t not in typemap:
            typemap[t] = []
        if not device_exists(xenify(dev)):
            continue
        typemap[t] += [xenify(dev)]

    # One of the desired types not found: The other type has it all
    if "ebs" not in typemap and "ephemeral" not in typemap:
        sys.stderr.write("No disks found\n")
        sys.exit(0)
    elif "ebs" not in typemap:
        role["ephemeral"] = "all"
    elif "ephemeral" not in typemap:
        role["ebs"] = "all"

    # Could happen even if properly invoked through ds2 if one of the
    # types is not present, and the other is set to "unused"
    if role["ebs"] == role["ephemeral"]:
        err_msg = "Exception when parsing config. Both EBS and ephemeral are set to the same role (%s)"
        raise Exception(err_msg % (role["ebs"]))

    # If one type configured for all, the other for a specified role, and both present:
    # That's valid and sane: respect that and mount one on top of the other. We just need
    # make sure that the root is mounted first.
    order = list(typemap.keys())
    order.sort()

    mdidx = 0
    for t in order:
        config_array(typemap[t], role[t], mdidx)
        mdidx += 1

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Disk creation script for Scylla.')
    parser.add_argument('--scylla-data-root', dest='scylla_data_root', action='store',
                        help='location of Scylla root data directory', default="/var/lib/scylla")
    args = parser.parse_args()

    scylla_root = args.scylla_data_root

    get_disk_bundles()
