#!/usr/bin/env python3
#
# Copyright 2022 ScyllaDB
#
# SPDX-License-Identifier: Apache-2.0

import functools
import json
import logging
import os
import re
import time
import urllib.error
import urllib.parse
import urllib.request
import psutil
import socket
import glob
import distro
import base64
import datetime
import asyncio
from subprocess import run, CalledProcessError
from abc import ABCMeta, abstractmethod


def out(cmd, shell=True, timeout=None, encoding='utf-8', ignore_error=False, user=None, group=None):
    try:
        res = run(cmd, capture_output=True, shell=shell, timeout=timeout, check=not ignore_error, encoding=encoding, user=user, group=group)
    except CalledProcessError as e:
        print(f'''
Command '{cmd}' returned non-zero exit status: {e.returncode}
----------  stdout  ----------
{e.stdout.strip()}
------------------------------
----------  stderr  ----------
{e.stderr.strip()}
------------------------------
'''[1:-1])
        raise
    return res.stdout.strip()


import sys
import traceback
import traceback_with_variables
import logging


def scylla_excepthook(etype, value, tb):
    os.makedirs('/var/tmp/scylla', mode=0o755, exist_ok=True)
    traceback.print_exception(etype, value, tb)
    exc_logger = logging.getLogger(__name__)
    exc_logger.setLevel(logging.DEBUG)
    exc_logger_file = f'/var/tmp/scylla/{os.path.basename(sys.argv[0])}-{os.getpid()}-debug.log'
    exc_logger.addHandler(logging.FileHandler(exc_logger_file))
    traceback_with_variables.print_exc(e=value, file_=traceback_with_variables.LoggerAsFile(exc_logger))
    print(f'Debug log created: {exc_logger_file}')

sys.excepthook = scylla_excepthook


def _curl_one(url, headers=None, method=None, byte=False, timeout=3):
    req = urllib.request.Request(url, headers=headers or {}, method=method)
    with urllib.request.urlopen(req, timeout=timeout) as res:
        if byte:
            return res.read()
        else:
            return res.read().decode('utf-8')

# @param headers dict of k:v
def curl(url, headers=None, method=None, byte=False, timeout=3, max_retries=5, retry_interval=5):
    retries = 0
    while True:
        try:
            return _curl_one(url, headers, method, byte, timeout)
        except (urllib.error.URLError, socket.timeout) as e:
            # if HTTP error code is client error (400-499), skip retrying
            if isinstance(e, urllib.error.HTTPError) and e.code in range(400,500):
                raise
            time.sleep(retry_interval)
            retries += 1
            if retries >= max_retries:
                raise

async def aiocurl(url, headers=None, method=None, byte=False, timeout=3, max_retries=5, retry_interval=5):
    retries = 0
    while True:
        try:
            return _curl_one(url, headers, method, byte, timeout)
        except (urllib.error.URLError, socket.timeout) as e:
            # if HTTP error code is client error (400-499), skip retrying
            if isinstance(e, urllib.error.HTTPError) and e.code in range(400,500):
                raise
            await asyncio.sleep(retry_interval)
            retries += 1
            if retries >= max_retries:
                raise

def read_one_line(filename):
    try:
        with open(filename) as f:
            return f.read().strip()
    except FileNotFoundError:
        return ''

class cloud_instance(metaclass=ABCMeta):
    @abstractmethod
    def get_local_disks(self):
        pass

    @abstractmethod
    def get_remote_disks(self):
        pass

    @abstractmethod
    def private_ipv4(self):
        pass

    @abstractmethod
    def is_supported_instance_class(self):
        pass

    @abstractmethod
    def is_dev_instance_type(self):
        pass

    @property
    @abstractmethod
    def instancetype(self):
        pass

    @abstractmethod
    def io_setup(self):
        pass

    @staticmethod
    @abstractmethod
    def check():
        pass

    @property
    @abstractmethod
    def user_data(self):
        pass

    @property
    @abstractmethod
    def nvme_disk_count(self):
        pass

    @property
    @abstractmethod
    def endpoint_snitch(self):
        pass

    @classmethod
    @abstractmethod
    def identify_dmi(cls):
        pass

    @classmethod
    @abstractmethod
    async def identify_metadata(cls):
        pass

    @classmethod
    async def identify(cls):
        return cls.identify_dmi() or await cls.identify_metadata()



class gcp_instance(cloud_instance):
    """Describe several aspects of the current GCP instance"""

    EPHEMERAL = "ephemeral"
    PERSISTENT = "persistent"
    ROOT = "root"
    META_DATA_BASE_URL = "http://metadata.google.internal/computeMetadata/v1/instance/"
    ENDPOINT_SNITCH = "GoogleCloudSnitch"

    def __init__(self):
        self.__type = None
        self.__cpu = None
        self.__memoryGB = None
        self.__nvmeDiskCount = None
        self.__firstNvmeSize = None
        self.__osDisks = None

    @property
    def endpoint_snitch(self):
        return self.ENDPOINT_SNITCH


    @classmethod
    def identify_dmi(cls):
        product_name = read_one_line('/sys/class/dmi/id/product_name')
        if product_name == "Google Compute Engine":
            return cls
        return None

    @classmethod
    async def identify_metadata(cls):
        """Check if it's GCE instance via DNS lookup to metadata server."""
        try:
            addrlist = socket.getaddrinfo('metadata.google.internal', 80)
        except socket.gaierror:
            return None
        for res in addrlist:
            af, socktype, proto, canonname, sa = res
            if af == socket.AF_INET:
                addr, port = sa
                if addr == "169.254.169.254":
                    # Make sure it is not on GKE
                    try:
                        await aiocurl(cls.META_DATA_BASE_URL + "machine-type?recursive=false",
                    headers={"Metadata-Flavor": "Google"})
                    except urllib.error.HTTPError:
                        return None
                    return cls
        return None


    def __instance_metadata(self, path, recursive=False):
        return curl(self.META_DATA_BASE_URL + path + "?recursive=%s" % str(recursive).lower(),
                    headers={"Metadata-Flavor": "Google"})

    def is_in_root_devs(self, x, root_devs):
        for root_dev in root_devs:
            if root_dev.startswith(os.path.join("/dev/", x)):
                return True
        return False

    def _non_root_nvmes(self):
        """get list of nvme disks from os, filter away if one of them is root"""
        nvme_re = re.compile(r"nvme\d+n\d+$")

        root_dev_candidates = [x for x in psutil.disk_partitions() if x.mountpoint == "/"]

        root_devs = [x.device for x in root_dev_candidates]

        nvmes_present = list(filter(nvme_re.match, os.listdir("/dev")))
        return {self.ROOT: root_devs, self.EPHEMERAL: [x for x in nvmes_present if not self.is_in_root_devs(x, root_devs)]}

    def _non_root_disks(self):
        """get list of disks from os, filter away if one of them is root"""
        disk_re = re.compile(r"/dev/sd[b-z]+$")

        root_dev_candidates = [x for x in psutil.disk_partitions() if x.mountpoint == "/"]

        root_devs = [x.device for x in root_dev_candidates]

        disks_present = list(filter(disk_re.match, glob.glob("/dev/sd*")))
        return {self.PERSISTENT: [x.lstrip('/dev/') for x in disks_present if not self.is_in_root_devs(x.lstrip('/dev/'), root_devs)]}

    @property
    def os_disks(self):
        """populate disks from /dev/ and root mountpoint"""
        if self.__osDisks is None:
            __osDisks = {}
            nvmes_present = self._non_root_nvmes()
            for k, v in nvmes_present.items():
                __osDisks[k] = v
            disks_present = self._non_root_disks()
            for k, v in disks_present.items():
                __osDisks[k] = v
            self.__osDisks = __osDisks
        return self.__osDisks

    def get_local_disks(self):
        """return just transient disks"""
        return self.os_disks[self.EPHEMERAL]

    def get_remote_disks(self):
        """return just persistent disks"""
        return self.os_disks[self.PERSISTENT]

    @staticmethod
    def isNVME(gcpdiskobj):
        """check if disk from GCP metadata is a NVME disk"""
        if gcpdiskobj["interface"]=="NVME":
            return True
        return False

    def __get_nvme_disks_from_metadata(self):
        """get list of nvme disks from metadata server"""
        try:
            disksREST=self.__instance_metadata("disks", True)
            disksobj=json.loads(disksREST)
            nvmedisks=list(filter(self.isNVME, disksobj))
        except Exception as e:
            print ("Problem when parsing disks from metadata:")
            print (e)
            nvmedisks={}
        return nvmedisks

    @property
    def nvme_disk_count(self):
        """get # of nvme disks available for scylla raid"""
        if self.__nvmeDiskCount is None:
            try:
                ephemeral_disks = self.get_local_disks()
                count_os_disks=len(ephemeral_disks)
            except Exception as e:
                print ("Problem when parsing disks from OS:")
                print (e)
                count_os_disks=0
            nvme_metadata_disks = self.__get_nvme_disks_from_metadata()
            count_metadata_nvme_disks=len(nvme_metadata_disks)
            self.__nvmeDiskCount = count_os_disks if count_os_disks<count_metadata_nvme_disks else count_metadata_nvme_disks
        return self.__nvmeDiskCount

    @property
    def instancetype(self):
        """return the type of this instance, e.g. n2-standard-2"""
        if self.__type is None:
            self.__type = self.__instance_metadata("machine-type").split("/")[-1]
        return self.__type

    @property
    def cpu(self):
        """return the # of cpus of this instance"""
        if self.__cpu is None:
            self.__cpu = psutil.cpu_count()
        return self.__cpu

    @property
    def memoryGB(self):
        """return the size of memory in GB of this instance"""
        if self.__memoryGB is None:
            self.__memoryGB = psutil.virtual_memory().total/1024/1024/1024
        return self.__memoryGB

    def instance_size(self):
        """Returns the size of the instance we are running in. i.e.: 2"""
        instancetypesplit = self.instancetype.split("-")
        return instancetypesplit[2] if len(instancetypesplit)>2 else 0

    def instance_class(self):
        """Returns the class of the instance we are running in. i.e.: n2"""
        return self.instancetype.split("-")[0]

    def instance_purpose(self):
        """Returns the purpose of the instance we are running in. i.e.: standard"""
        return self.instancetype.split("-")[1]

    m1supported="m1-megamem-96" #this is the only exception of supported m1 as per https://cloud.google.com/compute/docs/machine-types#m1_machine_types

    def is_unsupported_instance_class(self):
        """Returns if this instance type belongs to unsupported ones for nvmes"""
        if self.instancetype == self.m1supported:
            return False
        if self.instance_class() in ['e2', 'f1', 'g1', 'm2', 'm1']:
            return True
        return False

    def is_supported_instance_class(self):
        """Returns if this instance type belongs to supported ones for nvmes"""
        if self.instancetype == self.m1supported:
            return True
        if self.instance_class() in ['n1', 'n2', 'n2d' ,'c2']:
            return True
        return False

    def is_recommended_instance_size(self):
        """if this instance has at least 2 cpus, it has a recommended size"""
        if int(self.instance_size()) > 1:
            return True
        return False

    def is_dev_instance_type(self):
        if self.instancetype in ['e2-micro', 'e2-small', 'e2-medium']:
            return True
        return False

    @staticmethod
    def get_file_size_by_seek(filename):
        "Get the file size by seeking at end"
        fd= os.open(filename, os.O_RDONLY)
        try:
            return os.lseek(fd, 0, os.SEEK_END)
        finally:
            os.close(fd)

    # note that GCP has 3TB physical devices actually, which they break into smaller 375GB disks and share the same mem with multiple machines
    # this is a reference value, disk size shouldn't be lower than that
    GCP_NVME_DISK_SIZE_2020=375

    @property
    def firstNvmeSize(self):
        """return the size of first non root NVME disk in GB"""
        if self.__firstNvmeSize is None:
            ephemeral_disks = self.get_local_disks()
            if len(ephemeral_disks) > 0:
                firstDisk = ephemeral_disks[0]
                firstDiskSize = self.get_file_size_by_seek(os.path.join("/dev/", firstDisk))
                firstDiskSizeGB = firstDiskSize/1024/1024/1024
                if firstDiskSizeGB >= self.GCP_NVME_DISK_SIZE_2020:
                    self.__firstNvmeSize = firstDiskSizeGB
                else:
                    self.__firstNvmeSize = 0
                    logging.warning("First nvme is smaller than lowest expected size. ".format(firstDisk))
            else:
                self.__firstNvmeSize = 0
        return self.__firstNvmeSize

    def is_recommended_instance(self):
        if not self.is_unsupported_instance_class() and self.is_supported_instance_class() and self.is_recommended_instance_size():
            # at least 1:2GB cpu:ram ratio , GCP is at 1:4, so this should be fine
            if self.cpu/self.memoryGB < 0.5:
                diskCount = self.nvme_disk_count
                # to reach max performance for > 16 disks we mandate 32 or more vcpus
                # https://cloud.google.com/compute/docs/disks/local-ssd#performance
                if diskCount >= 16 and self.cpu < 32:
                    logging.warning(
                        "This machine doesn't have enough CPUs for allocated number of NVMEs (at least 32 cpus for >=16 disks). Performance will suffer.")
                if diskCount < 1:
                    logging.warning("No ephemeral disks were found.")
                    return False
                diskSize = self.firstNvmeSize
                max_disktoramratio = 105
                # 30:1 Disk/RAM ratio must be kept at least(AWS), we relax this a little bit
                # on GCP we are OK with {max_disktoramratio}:1 , n1-standard-2 can cope with 1 disk, not more
                disktoramratio = (diskCount * diskSize) / self.memoryGB
                if (disktoramratio > max_disktoramratio):
                    logging.warning(
                        f"Instance disk-to-RAM ratio is {disktoramratio}, which is higher than the recommended ratio {max_disktoramratio}. Performance may suffer.")
                    return False
                return True
            else:
                logging.warning("At least 2G of RAM per CPU is needed. Performance will suffer.")
        return False

    def private_ipv4(self):
        return self.__instance_metadata("network-interfaces/0/ip")

    @staticmethod
    def check():
        pass

    def io_setup(self):
        run('/opt/scylladb/scylla-machine-image/scylla_cloud_io_setup', check=True, shell=True)

    @property
    def user_data(self):
        try:
            return self.__instance_metadata("attributes/user-data")
        except urllib.error.HTTPError:  # empty user-data
            return ""


class azure_instance(cloud_instance):
    """Describe several aspects of the current Azure instance"""

    EPHEMERAL = "ephemeral"
    PERSISTENT = "persistent"
    SWAP = "swap"
    ROOT = "root"
    ENDPOINT_SNITCH = "AzureSnitch"
    META_DATA_BASE_URL = "http://169.254.169.254/metadata/instance"

    def __init__(self):
        self.__type = None
        self.__cpu = None
        self.__location = None
        self.__zone = None
        self.__memoryGB = None
        self.__nvmeDiskCount = None
        self.__firstNvmeSize = None
        self.__osDisks = None

    @property
    def endpoint_snitch(self):
        return self.ENDPOINT_SNITCH

    @classmethod
    def identify_dmi(cls):
        # On Azure, we cannot discriminate between Azure and baremetal Hyper-V
        # from DMI.
        # But only Azure has waagent, so we can use it for Azure detection.
        sys_vendor = read_one_line('/sys/class/dmi/id/sys_vendor')
        if sys_vendor == "Microsoft Corporation" and os.path.exists('/etc/waagent.conf'):
            return cls
        return None

    @classmethod
    async def identify_metadata(cls):
        """Check if it's Azure instance via query to metadata server."""
        try:
            await aiocurl(cls.META_DATA_BASE_URL + cls.API_VERSION + "&format=text", headers = { "Metadata": "True" })
            return cls
        except (urllib.error.URLError, urllib.error.HTTPError):
            return None

# as per https://docs.microsoft.com/en-us/azure/virtual-machines/windows/instance-metadata-service?tabs=windows#supported-api-versions
    API_VERSION = "?api-version=2021-01-01"


    def __instance_metadata(self, path):
        """query Azure metadata server"""
        return curl(self.META_DATA_BASE_URL + path + self.API_VERSION + "&format=text", headers = { "Metadata": "True" })

    def is_in_root_devs(self, x, root_devs):
        for root_dev in root_devs:
            if root_dev.startswith(os.path.join("/dev/", x)):
                return True
        return False

    def _non_root_nvmes(self):
        """get list of nvme disks from os, filter away if one of them is root"""
        nvme_re = re.compile(r"nvme\d+n\d+$")

        root_dev_candidates = [x for x in psutil.disk_partitions() if x.mountpoint == "/"]
        if len(root_dev_candidates) != 1:
            raise Exception("found more than one disk mounted at root ".format(root_dev_candidates))

        root_devs = [x.device for x in root_dev_candidates]

        nvmes_present = list(filter(nvme_re.match, os.listdir("/dev")))
        return {self.ROOT: root_devs, self.EPHEMERAL: [x for x in nvmes_present if not self.is_in_root_devs(x, root_devs)]}

    def _get_swap_dev(self):
        if os.path.exists('/dev/disk/cloud/azure_resource'):
            return os.path.realpath('/dev/disk/cloud/azure_resource')
        else:
            return None

    def _non_root_disks(self):
        """get list of disks from os, filter away if one of them is root"""
        disk_re = re.compile(r"/dev/sd[b-z]+$")

        root_dev_candidates = [x for x in psutil.disk_partitions() if x.mountpoint == "/"]

        root_devs = [x.device for x in root_dev_candidates]

        disks_present = list(filter(disk_re.match, glob.glob("/dev/sd*")))
        swap_dev = self._get_swap_dev()
        swap = []
        if swap_dev:
            swap.append(swap_dev.lstrip('/dev/'))
        persistent = [x.lstrip('/dev/') for x in disks_present if not self.is_in_root_devs(x.lstrip('/dev/'), root_devs) and not x == swap_dev]
        return {self.PERSISTENT: persistent, self.SWAP: swap}

    @property
    def os_disks(self):
        """populate disks from /dev/ and root mountpoint"""
        if self.__osDisks is None:
            __osDisks = {}
            nvmes_present = self._non_root_nvmes()
            for k, v in nvmes_present.items():
                __osDisks[k] = v
            disks_present = self._non_root_disks()
            for k, v in disks_present.items():
                __osDisks[k] = v
            self.__osDisks = __osDisks
        return self.__osDisks

    def get_local_disks(self):
        """return just transient disks"""
        return self.os_disks[self.EPHEMERAL]

    def get_remote_disks(self):
        """return just persistent disks"""
        return self.os_disks[self.PERSISTENT]

    def get_swap_disks(self):
        return self.os_disks[self.SWAP]

    @property
    def nvme_disk_count(self):
        """get # of nvme disks available for scylla raid"""
        if self.__nvmeDiskCount is None:
            try:
                ephemeral_disks = self.get_local_disks()
                count_os_disks = len(ephemeral_disks)
            except Exception as e:
                print("Problem when parsing disks from OS:")
                print(e)
                count_os_disks = 0
            count_metadata_nvme_disks = self.__get_nvme_disks_count_from_metadata()
            self.__nvmeDiskCount = count_os_disks if count_os_disks < count_metadata_nvme_disks else count_metadata_nvme_disks
        return self.__nvmeDiskCount

    instanceToDiskCount = {
        "L8s": 1,
        "L16s": 2,
        "L32s": 4,
        "L48s": 6,
        "L64s": 8,
        "L80s": 10,
        "L8as": 1,
        "L16as": 2,
        "L32as": 4,
        "L48as": 6,
        "L64as": 8,
        "L80as": 10
    }

    def __get_nvme_disks_count_from_metadata(self):
        #storageProfile in VM metadata lacks the number of NVMEs, it's hardcoded based on VM type
        return self.instanceToDiskCount.get(self.instance_class(), 0)

    @property
    def instancelocation(self):
        """return the location of this instance, e.g. eastus"""
        if self.__location is None:
            self.__location = self.__instance_metadata("/compute/location")
        return self.__location

    @property
    def instancezone(self):
        """return the zone of this instance, e.g. 1"""
        if self.__zone is None:
            self.__zone = self.__instance_metadata("/compute/zone")
        return self.__zone

    @property
    def instancetype(self):
        """return the type of this instance, e.g. Standard_L8s_v2"""
        if self.__type is None:
            self.__type = self.__instance_metadata("/compute/vmSize")
        return self.__type

    @property
    def cpu(self):
        """return the # of cpus of this instance"""
        if self.__cpu is None:
            self.__cpu = psutil.cpu_count()
        return self.__cpu

    @property
    def memoryGB(self):
        """return the size of memory in GB of this instance"""
        if self.__memoryGB is None:
            self.__memoryGB = psutil.virtual_memory().total/1024/1024/1024
        return self.__memoryGB

    def instance_purpose(self):
        """Returns the class of the instance we are running in. i.e.: Standard"""
        return self.instancetype.split("_")[0]

    def instance_class(self):
        """Returns the purpose of the instance we are running in. i.e.: L8s"""
        return self.instancetype.split("_")[1]

    def is_supported_instance_class(self):
        """Returns if this instance type belongs to supported ones for nvmes"""
        if self.instance_class() in list(self.instanceToDiskCount.keys()):
            return True
        return False

    def is_recommended_instance_size(self):
        """if this instance has at least 2 cpus, it has a recommended size"""
        if self.cpu > 1:
            return True
        return False

    def is_recommended_instance(self):
        if self.is_supported_instance_class():
            return True
        return False

    def is_dev_instance_type(self):
        return False

    def private_ipv4(self):
        return self.__instance_metadata("/network/interface/0/ipv4/ipAddress/0/privateIpAddress")

    @staticmethod
    def check():
        pass

    def io_setup(self):
        run('/opt/scylladb/scylla-machine-image/scylla_cloud_io_setup', check=True, shell=True)

    @property
    def user_data(self):
        encoded_user_data = self.__instance_metadata("/compute/userData")
        if not encoded_user_data:
            return ''
        return base64.b64decode(encoded_user_data).decode()


class aws_instance(cloud_instance):
    """Describe several aspects of the current AWS instance"""
    META_DATA_BASE_URL = "http://169.254.169.254/latest/"
    ENDPOINT_SNITCH = "Ec2Snitch"
    METADATA_TOKEN_TTL = 21600

    def __disk_name(self, dev):
        name = re.compile(r"(?:/dev/)?(?P<devname>[a-zA-Z]+)\d*")
        return name.search(dev).group("devname")

    def __refresh_metadata_token(self):
        self._metadata_token_time = datetime.datetime.now()
        self._metadata_token = curl(self.META_DATA_BASE_URL + "api/token", headers={"X-aws-ec2-metadata-token-ttl-seconds": self.METADATA_TOKEN_TTL}, method="PUT")

    def __instance_metadata(self, path):
        if not self._metadata_token:
            self.__refresh_metadata_token()
        else:
            time_diff = datetime.datetime.now() - self._metadata_token_time
            time_diff_sec = int(time_diff.total_seconds())
            if time_diff_sec >= self.METADATA_TOKEN_TTL - 120:
                self.__refresh_metadata_token()
        return curl(self.META_DATA_BASE_URL + path, headers={"X-aws-ec2-metadata-token": self._metadata_token})

    def __device_exists(self, dev):
        if dev[0:4] != "/dev":
            dev = "/dev/%s" % dev
        return os.path.exists(dev)

    def __xenify(self, devname):
        dev = self.__instance_metadata('meta-data/block-device-mapping/' + devname)
        return dev.replace("sd", "xvd")

    def __filter_nvmes(self, dev, dev_type):
        nvme_re = re.compile(r"(nvme\d+)n\d+$")
        match = nvme_re.match(dev)
        if not match:
            return False
        nvme_name = match.group(1)
        with open(f'/sys/class/nvme/{nvme_name}/model') as f:
            model = f.read().strip()
        if dev_type == 'ephemeral':
            return model != 'Amazon Elastic Block Store'
        else:
            return model == 'Amazon Elastic Block Store'

    def _non_root_nvmes(self):
        nvme_re = re.compile(r"nvme\d+n\d+$")

        root_dev_candidates = [ x for x in psutil.disk_partitions() if x.mountpoint == "/" ]
        if len(root_dev_candidates) != 1:
            raise Exception("found more than one disk mounted at root'".format(root_dev_candidates))

        root_dev = root_dev_candidates[0].device
        if root_dev == '/dev/root':
            root_dev = out('findmnt -n -o SOURCE /')
        ephemeral_present = list(filter(lambda x: self.__filter_nvmes(x, 'ephemeral'), os.listdir("/dev")))
        ebs_present = list(filter(lambda x: self.__filter_nvmes(x, 'ebs'), os.listdir("/dev")))
        return {"root": [ root_dev ], "ephemeral": ephemeral_present, "ebs": [ x for x in ebs_present if not root_dev.startswith(os.path.join("/dev/", x))] }

    def __populate_disks(self):
        devmap = self.__instance_metadata("meta-data/block-device-mapping")
        self._disks = {}
        devname = re.compile("^\D+")
        nvmes_present = self._non_root_nvmes()
        for k,v in nvmes_present.items():
            self._disks[k] = v

        for dev in devmap.splitlines():
            t = devname.match(dev).group()
            if t == "ephemeral" and nvmes_present:
                continue
            if t not in self._disks:
                self._disks[t] = []
            if not self.__device_exists(self.__xenify(dev)):
                continue
            self._disks[t] += [self.__xenify(dev)]
        if not 'ebs' in self._disks:
            self._disks['ebs'] = []

    def __mac_address(self, nic='eth0'):
        with open('/sys/class/net/{}/address'.format(nic)) as f:
            return f.read().strip()

    def __init__(self):
        self._metadata_token = None
        self._metadata_token_time = None
        self._type = self.__instance_metadata("meta-data/instance-type")
        self.__populate_disks()

    @property
    def endpoint_snitch(self):
        return self.ENDPOINT_SNITCH


    @classmethod
    def identify_dmi(cls):
        product_version = read_one_line('/sys/class/dmi/id/product_version')
        bios_vendor = read_one_line('/sys/class/dmi/id/bios_vendor')
        # On Xen instance, product_version is like "4.11.amazon"
        if product_version.endswith('.amazon'):
            return cls
        # On Nitro instance / Baremetal instance, bios_vendor is "Amazon EC2"
        if bios_vendor == 'Amazon EC2':
            return cls
        return None

    @classmethod
    async def identify_metadata(cls):
        """Check if it's AWS instance via query to metadata server."""
        try:
            res = await aiocurl(cls.META_DATA_BASE_URL + "api/token", headers={"X-aws-ec2-metadata-token-ttl-seconds": cls.METADATA_TOKEN_TTL}, method="PUT")
            print(f'aws_instance: {res}')
            return cls
        except (urllib.error.URLError, urllib.error.HTTPError, socket.timeout):
            return None

    @property
    def instancetype(self):
        """Returns which instance we are running in. i.e.: i3.16xlarge"""
        return self._type

    def instance_size(self):
        """Returns the size of the instance we are running in. i.e.: 16xlarge"""
        return self._type.split(".")[1]

    def instance_class(self):
        """Returns the class of the instance we are running in. i.e.: i3"""
        return self._type.split(".")[0]

    def is_supported_instance_class(self):
        if self.instance_class() in ['i2', 'i3', 'i3en', 'c5d', 'm5d', 'm5ad', 'r5d', 'z1d', 'c6gd', 'm6gd', 'r6gd', 'x2gd', 'im4gn', 'is4gen', 'i4i', 'i4g', 'i7ie']:
            return True
        return False

    def is_dev_instance_type(self):
        if self.instancetype in ['t3.micro']:
            return True
        return False

    def get_en_interface_type(self):
        instance_class = self.instance_class()
        instance_size = self.instance_size()
        if instance_class in ['c3', 'c4', 'd2', 'i2', 'r3']:
            return 'ixgbevf'
        if instance_class in ['a1', 'c5', 'c5a', 'c5d', 'c5n', 'c6g', 'c6gd', 'f1', 'g3', 'g4', 'h1', 'i3', 'i3en', 'inf1', 'm5', 'm5a', 'm5ad', 'm5d', 'm5dn', 'm5n', 'm6g', 'm6gd', 'p2', 'p3', 'r4', 'r5', 'r5a', 'r5ad', 'r5b', 'r5d', 'r5dn', 'r5n', 't3', 't3a', 'u-6tb1', 'u-9tb1', 'u-12tb1', 'u-18tn1', 'u-24tb1', 'x1', 'x1e', 'z1d', 'c6g', 'c6gd', 'm6g', 'm6gd', 't4g', 'r6g', 'r6gd', 'x2gd', 'im4gn', 'is4gen', 'i4i', 'i4g', 'i7ie']:
            return 'ena'
        if instance_class == 'm4':
            if instance_size == '16xlarge':
                return 'ena'
            else:
                return 'ixgbevf'
        return None

    def disks(self):
        """Returns all disks in the system, as visible from the AWS registry"""
        disks = set()
        for v in list(self._disks.values()):
            disks = disks.union([self.__disk_name(x) for x in v])
        return disks

    def root_device(self):
        """Returns the device being used for root data. Unlike root_disk(),
           which will return a device name (i.e. xvda), this function will return
           the full path to the root partition as returned by the AWS instance
           metadata registry"""
        return set(self._disks["root"])

    def root_disk(self):
        """Returns the disk used for the root partition"""
        return self.__disk_name(self._disks["root"][0])

    def non_root_disks(self):
        """Returns all attached disks but root. Include ephemeral and EBS devices"""
        return set(self._disks["ephemeral"] + self._disks["ebs"])

    @property
    def nvme_disk_count(self):
        return len(self.non_root_disks())

    def get_local_disks(self):
        """Returns all ephemeral disks. Include standard SSDs and NVMe"""
        return self._disks["ephemeral"]

    def get_remote_disks(self):
        """Returns all EBS disks"""
        return self._disks["ebs"]

    def public_ipv4(self):
        """Returns the public IPv4 address of this instance"""
        return self.__instance_metadata("meta-data/public-ipv4")

    def private_ipv4(self):
        """Returns the private IPv4 address of this instance"""
        return self.__instance_metadata("meta-data/local-ipv4")

    def is_vpc_enabled(self, nic='eth0'):
        mac = self.__mac_address(nic)
        mac_stat = self.__instance_metadata('meta-data/network/interfaces/macs/{}'.format(mac))
        return True if re.search(r'^vpc-id$', mac_stat, flags=re.MULTILINE) else False

    @staticmethod
    def check():
        return run('/opt/scylladb/scylla-machine-image/scylla_ec2_check --nic eth0', shell=True)

    def io_setup(self):
        run('/opt/scylladb/scylla-machine-image/scylla_cloud_io_setup', check=True, shell=True)

    @property
    def user_data(self):
        base_contents = self.__instance_metadata('').splitlines()
        if 'user-data' in base_contents:
            return self.__instance_metadata('user-data')
        else:
            return ''


async def identify_cloud_async():
    tasks = [
        asyncio.create_task(aws_instance.identify()),
        asyncio.create_task(gcp_instance.identify()),
        asyncio.create_task(azure_instance.identify())
    ]
    result = None
    for task in asyncio.as_completed(tasks):
        result = await task
        if result:
            for other_task in tasks:
                other_task.cancel()
            break
    return result


@functools.cache
def identify_cloud():
    time_start = datetime.datetime.now()
    result = asyncio.run(identify_cloud_async())
    time_end = datetime.datetime.now()
    return result


def is_ec2():
    return identify_cloud() == aws_instance

def is_gce():
    return identify_cloud() == gcp_instance

def is_azure():
    return identify_cloud() == azure_instance

def get_cloud_instance():
    cls = identify_cloud()
    return cls()


CONCOLORS = {'green': '\033[1;32m', 'red': '\033[1;31m', 'yellow': '\033[1;33m', 'nocolor': '\033[0m'}


def colorprint(msg, **kwargs):
    fmt = dict(CONCOLORS)
    fmt.update(kwargs)
    print(msg.format(**fmt))

def is_redhat_variant():
    return 'rhel' in distro.like().split()
